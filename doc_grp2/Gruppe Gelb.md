# Gruppe 2 - Gelb

Teilnehmer:

- Frauke J√∂rgens
- Jan Ottm√ºller
- Franz Wernicke
- Thorger Dittmann
- Felix Maa√ü (Gruppenleiter)


### Unsere Aufgabenstellung

- Motor- und Lenkregelung + Nothalt
- Fahrbahnerkennung (Gerade und Kurve)
- Kreuzungen erkennen und anhalten

----

## Tagebuch

### Mittwoch, 15.11.
**14 bis 17 Uhr - Alle:**  
Heute haben wir uns die Demos zu Lane- und Markerdetection angesehen.

Wir verwenden ab jetzt in unserer sample Lanedetection nur den Blau-Kanal, um das blaue Klebeband gut zu erkennen. Dies hat Hermann uns gegeben, um die Fahrbahnmarkierung zu erstellen.

Wir haben ein paar Streifen auf einen Leitz-Ordner aufgeklebt und es damit getestet.
Wir w√ºrden aber gern in Zukunft von RGB auf HSV/HSB umsteigen, damit auch unterschiedliche Blaut√∂ne erkannt werden k√∂nnen (verschiedene Lichteinwirkung).
Darum haben wir versucht, einmal die Roh-Videodaten mittels OpenCV zu exportieren. Leider haben wir uns gut 45 Minuten daran aufgehangen, dass wir den falschen Codec (HFUV) verwendet haben und es so nie zu einer Datei kam.
Schlie√ülich benutzen wir nun Motion-JPG (MJPG), welches einwandfrei funktioniert.

Frauke m√∂chte sich gern eine Testaufnahme mal mit diskretem OpenCV Code (ohne ADTF drumherum) ansehen.

Dar√ºber hinaus hat Franz sich einmal die Konfiguration der Radsensoren bzw. der Motorsteuerung genauer angeschaut.

Wir sind in der Lage die Demo zum Laufen zu bringen und es erscheinen einige Diagramme. Darunter unter anderem auch der zur√ºckgelegte Fahrtweg. Leider hat die Ordinate keine Einheit und 4000m/s sind dann doch etwas viel ;-)


----


### Donnerstag, 16.11.
**17 bis 18 Uhr - Franz, Frauke, Jan, Felix:**  
Heute wollten wir f√ºr Franz einmal richtige Werte des Autos aufnehmen, dass hei√üt Spannungen, USS, Lenkwinkel, etc..., sodass er sie zuhause abspielen kann.

Leider war es uns nicht m√∂glich, die Motoren zum laufen zu bringen. Wir haben versucht, die Fernbedienung neu zu koppeln. Dies verlief einwandfrei, **jedoch tat sich nichts**.

Alle Steckkontakte waren unseren Erachtens richtig und der Akku hatte auch 8,16 Volt. Die Sicherung war intakt. Dennoch wollte weder Servo noch Antrieb reagieren.

Felix hat Hermann dar√ºber bereits informiert.

W√§hrend Franz und Felix diese ganzen Dinge pr√ºften, haben Frauke und Jan die **Linienerkennung** √ºber den HSV-Farbraum (an welchem Jan zuhause gearbeitet hatte) fertiggestellt. Wir haben die Linienerkennung jetzt also soweit fertig, dass wir das blaue Klebeband vom Hintergrund unter bekannten Lichtverh√§ltnissen sehr gut **(Danke Jan üòá)** vom Rest trennen k√∂nnen. Mit unserem Ordner-Test waren die Ergebnisse einwandfrei!

Daraufhin haben wir (Franz, Frauke und Felix) uns daran gesetzt, dass das git repo zus√§tzlich die ADTF-Konfigurationen mit beinhaltet.

**Der erste Versuch**, es per `.gitignore` plus Whitelisting (um die ganzen anderen Ordner nicht angeben zu m√ºssen) zu l√∂sen, schlug fehl. Die `.gitignore` Datei war auch nach mehreren Versuchen noch nicht richtig konfiguriert. Es wurden lediglich die Top-Level Dateien der gewhitelisteten Ordner hinzugef√ºgt.

**Der zweite Versuch**, die gew√ºnschten Ordner auszulagern und per **symlink** wieder in f√ºr ADTF an der gew√ºnschten Stelle einzubinden, ging zun√§chst super. Die Dateien wurden gefunden und man konnte das Projekt ausf√ºhren.
Leider hatte wir ab da jedoch Probleme mit den **cmakefiles**, da sie den Build-Ordner nicht mehr finden konnten. Da in Zukunft hiermit h√∂chstwahrscheinlich noch mehr Probleme aufgetreten w√§ren, haben wir diese "L√∂sung" beerdigt.

**Zuletzt** haben Frauke und Felix also doch nochmal die `.gitignore`s ausgepackt und viel trial-and-error angewendet. Dies f√ºhrte uns dann irgendwann dazu, dass git beim Hinzuf√ºgen in einer Endlosschleife zu stecken schien.  
Wir haben uns den .git Ordner mal genauer angesehen und nach Anomalien gesucht. Dieser war inzwischen 1,7GB gro√ü. Sehr merkw√ºrdig.  
Wir suchten aber weiterhin die Doku ab und sind so von einer gitignore Konfiguration zur N√§chsten gestapft.

Wir haben den Ordner gel√∂scht und ein neues repo initialisiert. **Gleicher Fehler**: `git add` f√ºhrt zu keiner Ausgabe.

Tats√§chlich sind wir erst nach ca. eine Stunde suchen der Dateigr√∂√üe des repos auf den Grund gegangen. Es war das Problem, dass wir **3GB Zeitaufnahmen** gemacht hatten, und git diese auch zur Versionierung hinzuf√ºgen wollte. Dass dies etwas dauern kann, ist verst√§ndlich.

Letztendlich haben wir diese Dateien gel√∂scht und siehe da: Unsere `.gitignore` Datei war wohl doch nicht falsch. Die "Endlosschleife" war Vergangenheit.

Auf solch ein Problem f√§llt man wohl nur einmal rein ;-)

19:45 haben wir dann schlie√ülich das Licht ausschalten k√∂nnen.


----


### Freitag, 17.11.
**11:00 bis 11:35 Uhr - Hermann, Franz, Felix:**  
Heute wieder ans Auto gesetzt und uns auf Fehlersuche begeben. Diesmal war Hermann dabei. Wir haben ihm die Probleme erneut geschildert. Wir haben die Kontakte der Einspeisung, der Sicherung und die des Abnehmers gepr√ºft.
Zwischen den Polen von Sicherung und Abnehmer war kein wirklicher Durchgang zu messen (60kOhm).

**12 bis 12:30 Uhr - Timm Bostelmann, Hermann, Franz, Frauke, Felix:**  
Nach dem Essen holten wir uns Timm, den Modellbauexperten hinzu. Wir haben den Akku direkt mit dem Motorsteuerger√§t verbunden und es leuchtete auf. **Bingo! Es lag nicht an einem frittierten Steuerger√§t.**

Insofern gingen wir davon aus, dass das PCB einen Schlag weg hatte, aber tats√§chlich stellte sich nach einem **Anruf beim Hersteller**, als dieser die Teilenummer des Boards wissen wollte, **zuf√§lligerweise** heraus, dass der Arduino nicht richtig sa√ü und Hermann ihn nur wieder richtig in den Sockel dr√ºcken musste, damit alles wieder lief.

**15:00 bis 15:30 Uhr - Franz, Frauke, Felix (+ Lukas und Hendrik):**  
Wir haben Lukas und Hendrik, zwei Interessierte, bei Vortr√§gen zu KI aufgeschnappt. Wir erkl√§rten ihnen, wie das Modell in Zusammenspiel mit dem Fahrzeug funktioniert oder halt eben nicht funktinoiert ;-).
Au√üerdem haben wir unsere gew√ºnschte Aufgabenstellung ein Mal etwas konkreter formuliert.

**15:30 bis 17:45 Uhr - Franz, Felix (+ Lukas):**  

Letztendlich haben Franz und ich an der Motorsteuerung gebastelt. Wir fanden heraus, dass die `USSStructs` und `USSStructPlaus` keine Unterschiede aufweisen und auch nie im Code referenziert werden. **Das dann zu Plausibil√§tspr√ºfung ;-)**

Des Weiteren ist uns aufgefallen, dass die Wert,e die aus dem `Wheel Converter` kommen, **exponentiell wachsen**, die Geschwindigkeit zur Raddrehzahl und die zur√ºckgelegte Distanz bei konstanter Geschwindigkeit.

Wir haben den Durchmesser der Reifen neu bestimmt und die Konstanten des Umfangs im Code abge√§ndert.

Felix konzentrierte sich nebenbei auf die Bedienung und **Kommunikation mit dem Auto via VNC**. Wir k√∂nnen einen der anderen Robotik-PCs verwenden und dort `TigerVNC` installieren. F√ºr den Host (das Auto) k√∂nnen wir `x11vnc` verwenden.

Ein Test auf Felix' virtueller Maschiene war erfolgreich :).


----


### Montag, 20.11.
**14:00 bis 17:30 Uhr - Frauke, Franz, Felix, Thorger:**  
Codecs - OpenCV wurde nicht mit FFMPEG Support gebuilded, deswegen kein `HFYU`-Codec. In Zukunft verwenden wir also `MJPG`.

Franz hat uns Zugriff √ºber SSH Keys zu Github verschafft. Daf√ºr muss noch ein Whitelisting f√ºr SSH des Robolabs erfolgen. Tempor√§r verwenden wir ein Handy als Hotspot oder klinken uns per LAN ins Mitarbeiter-Netzwerk ein.

Wir verwenden nun `TightVNC` als Client und `x11vnc` als Server.
Ein Problem ist noch, dass beim Ausst√∂pseln der Monitore der VNC Client auf einen Bildschirm limitiert wird.

R√ºckfahrkamera war kurzzeitig nicht verf√ºgbar. **Reboot tut gut**.


**17:30 bis 18:00 Uhr - Frauke, Franz, Felix:**  
Wir sind am Abend noch ein wenig mit dem Auto per Fernbedienung rumgefahren. Dabei hatten wir √ºber WLAN/VNC die Kontrolle √ºber Aufnahmen, etc.


**18:00 bis 19:10 Uhr - Franz, Felix:**  
Als wir dies nun alles zum Laufen gebracht hatten, klebten einigen blaue Streifen auf den Boden. Diese repr√§sentieren nun unsere Stra√üe. Die Fahrbahn ist nach Augenma√ü **einen Meter** breit.

Dann haben wir ein Szenario **'Links Abbiegen'** aufgenommen.


----


### Dienstag, 21.11.
**09:30 bis 10:55 Uhr - Frauke, Franz, Felix:**  

- Depth-Image-Processing
- bitwise_not (schwarz ist nah, wei√ü ist fern)
- Median-Filter (not working with radius of 7 and 16bit images)

**14:00 bis 15:30 Uhr - Alle:**  

- Position der RealSense Kamera + Halter Design
- Gedanken √ºber Zeitplanung
- Thorger's VM zum Laufen bringen
- Scope-Displays zeigen falsche Werte an: exponentielles Verhalten -> Konsole zeigt lineares Verhalten

**15:30 bis 17:15 Uhr - Jan:**

Jan hat ein Model f√ºr eine Halterung der RealSense Kamera in TinkerCAD gebaut, und mit kurzer Hilfe von Timm Bostelmann gedruckt. Leider ist die Passform etwas zu eng. Ein weiterer versuch, mit angepasstem Modell erfolgt warscheinlich morgen. 


----


### Mittwoch, 22.11.
**14:00 bis 15:30 Uhr - Jan, Franz, Felix:**  

Erneuter 3D-Druck mit angepasstem Model. Diesmal k√∂nnen wir erfreut von einem **'Snug-Fit'** sprechen. :-). Bei Bedarf kann man die RealSense Kamera nun hier anbringen und kann somit mehr von der Stra√üe vor dem Auto sehen. 

Jan hat begonnen sich mit der **Hough-Transformation** zu besch√§ftigen und sie auf 'Edges' im Kamerabild angewendet.
Erste Ergebnisse sind bereits erstaunlich gut.

Franz hat seine SSD an den Robotik-PC angeschlossen und kann dort mit Hardwarebeschleunigung die Aufnahmen auch fl√ºssig abspielen.
Leider meckert der Grafiktreiber etwas rum, was dazu f√ºhrt, dass der Desktop nicht erweitert sondern nur gespiegelt werden kann. **#NichtSoGeil :-(**

Felix schaut sich die Doku einzelner Filter an und macht sich weitere Gedanken zum Lenkregler.

**15:30 bis 17:30 Uhr - Franz, Felix:**  

Franz schraubt an einem Filter, der einfach nur einen Float-Wert ausgibt.
Diese Aufgabe klingt total trivial, ist aber alles andere als leicht zu l√∂sen: **Ein Hoch auf ADTF!**  
Die Dokumentationen von existieren Dateien ist zwar existent, leider entspricht sie jedoch nicht dem Code (Kopierpastete).  
**Merke:** *"Keine Doku ist immer noch besser als falsche Doku!"*

Felix hat sich w√§hrenddessen mit dem Lenkregler und dessen Outputs besch√§ftigt. Er stie√ü auf die selben Probleme.
Wir setzten uns also zun√§chst das Ziel, die Struktur der Filter √ºberhaupt erst einmal zu verstehen.

**17:30 bis 19:10 Uhr - Felix:**  
Als Franz dann verzweifelt das Weite gesucht hatte, warf Felix einen Blick in Hermann's Doku-Ordner zur Software. Tats√§chlich gab es einen Eintrag 'Schreib deinen ersten Filter'. Dieser bestand aber mehr oder weniger aus diesen Punkten:

1. Kopiere den Demo-Ordner
2. Bennenne ihn um
3. Benenne die enthaltenen Dateien um
4. Passe die CMake an
5. Viel Gl√ºck!

Insofern -> **Nicht sehr hilfreich :-(**  
Felix musste als noch etwas weiter an seinem Filter verzweifeln. Schlie√ülich ist ihm aufgefallen, dass die Daten√ºbertragung zwischen zwei Filtern sich einer Art komplizierter `Dictionaries` bedient.

Letztendlich ist es nur ein *"Nimm mal diese Daten (Void\*) und baller sie in den Eimer dort dr√ºben!"*. Formal sieht das dann wie folgt aus:

1. Gr√∂√üe der Daten mittels `IMediaSerializer` berechnen.
2. Speicher f√ºr ein `IMediaSample` alloziieren.
3. Einen `Write-Lock` holen. Man bekommt einen `IMediaCoderExt`
 zur√ºck.
4. Eine ID f√ºr den *'Eimer'*, in den die Daten geschrieben werden k√∂nnen, vom `IMediaCoderExt` besorgen.
5. Schreibe **diesen Wert** in **diesen Eimer**.
6. Setze die aktuelle Zeit in das `IMediaSample` ein, damit jeder wei√ü, wann das Paket zugeschn√ºrt wurde.
7. √úbertrage das `IMediaSample`.


----


### Donnerstag, 23.11.
**17:00 bis 18:30 Uhr - Felix:**  

**17:00 bis 19:15 - Franz und Frauke**

Da wir heute aufgrund des anstehenden Besuchs der Ministerin im Projektraum gearbeitet haben und wir unsere blaue Stra√üe nicht mitgenommen haben, mussten wir rot/orange Riesen-Steckerleisten zum Testen zu Rate ziehen. F√ºr die Erkennung der Farben mussten wir nur an den Parametern `hueLow` und `hueHigh` schrauben. Wir haben den Bereich zwischen 0 und 10¬∞ gew√§hlt.

Wir haben uns mit der GPU-Implementierung der Hough-Transformation besch√§ftigt. Es stellte sich heraus, dass das gar nicht so schwierig ist, man muss nur die richtigen Header (gute Kandidaten sind `<opencv2/cudaarithm.hpp>`, `<opencv2/core/cuda.hpp>` und `<opencv2/cudaimgproc.hpp>`) kennen und in vielen F√§llen nur die `cv::Mat`'s per `upload` auf die GPU laden. Ergebnis ist eine `cv::cuda::GpuMat`. Ist man fertig mit allen GPU-Operationen, muss die `cv::cuda::GpuMat` wieder heruntergeladen werden und auf eine `cv::Mat` geschrieben werden. Vor viele Funktionen muss man auch nur `cv::cuda::` + `Funktionsname` schreiben. Fies wird es, wenn das gerade nicht reicht, und sich die komplette Signatur √§ndert! So ist das zum Beispiel bei der Funktion `cv::HoughLines`. Die wird in der GPU-Implementierung durch einen Pointer auf eine abstrakte Klasse repr√§sentiert. Eine Variable dieses Typs sieht dann so aus: `cv::Ptr<cv::cuda::HoughLinesDetector> hough`. Auf diesen Pointer kann man dann beispielsweise die Funktion `detect` aufrufen. Beide Operationen zusammen haben eine √§hnliche Signatur wie die CPU-Implementierung von `cv::HoughLines`. Einige Funktionen gibt es auch nicht mit GPU-Unterst√ºtzung, zum Beispiel `cv::line`.

Nachdem alle Kompilier- und Laufzeitfehler behoben wurden, funktionierte es auch! **Und wir haben einen deutlichen Unterschied zwischen CPU- und GPU-Implementierung gemerkt**, sodass die GPU-Variante -- je nach Aufl√∂sung des Videos -- nahezu in Echtzeit abl√§uft. Dazu k√∂nnen wir aber gerne noch genauere Messungen vornehmen. Leider haben wir die CPU-Variante nicht committed, sodass wir die erst einmal wieder rekonstruieren m√ºssen :-(.

Jetzt steht nur noch die Frage im Raum, wie man damit arbeiten kann, wenn man keine nVidia-Grafikkarte, oder keinen station√§ren PC hat (wie Frauke). Implementiert man erst f√ºr die CPU, und muss man dann alles √§ndern? Oder implementiert man "blind" in der GPU-API und testet dann Live am Auto? Vielleicht haben wir uns damit doch selbst ins Bein geschossen...


----


### Freitag, 24.11.
**16:00 bis - Felix, Franz und Frauke:**

Wir haben Franz die GPU-Implementierung gezeigt, die Farbwerte wieder an Blau angepasst.

Unsere alten Werte waren:
```
hueHigh:    120
hueLow:     90
Saturation: 120
Value:      ??
```

Unsere neuen Werte sind:
```
hueHigh:    120
hueLow:     100
Saturation: 80
Value:      2
```
~~Unsere alten Werte haben wir wohl nie committed, deshalb sind sie verloren gegangen. Vielleicht wei√ü Jan sie noch ...~~ Frauke hat sie in einem Kommentar gefunden (bis auf `Value` -- siehe oben)!


----


### Montag, 27.11.
**10:00 bis 14:00 Uhr - Felix und Franz:**  
Fertigstellung des `FloatValueGenerator` und Fehlersuche f√ºr den `RadiusToAngleConverter`. Es lag letztendlich an einer nicht initialisierten Variablen :]

**14:00 bis 15:30 Uhr - Felix, Franz, Frauke und Jan:**  
Mapping von Angle auf ServoValue im `RadiusToAngleConverter` erstellt.
Frauke und Jan haben sich weiter mit der Fahrbahnerkennung besch√§ftigt. Es wurde ein Algorithmus zur B√ºndelung √§hnlicher Linien erstellt, da f√ºr eine Fahrbahnmarkierung mehrere Linien erkannt werden. Hierzu wird `cv::partition()` und eine eigene Hilfsmethode `isEqual()` verwendet. 

**15:30 bis 16:45 Uhr - Frauke und Jan:**  
S.o.

**18:30 bis 19:00 Uhr - Felix:**  
Test des `RadiusToAngleConverter`.
Inbetriebnahme des Fahrzeugs war geplant. Jedoch hat die Hardware versagt. Erst wollte Ubuntu nich mehr booten (im Recovery-Mode gings :]), danach klappte VNC nicht mehr.

Insofern muss der Test auf morgen verschoben werden.


----


### Dienstag, 28.11.
**21:00 bis 22:30 Uhr - Felix:**  
Felix hat den Converter in zwei Teile aufgetrennt. Wir k√∂nnen nun sowohl von einem **Radius zu einem Winkel**, als auch von einem **Winkel zu einem Servo-Stellwert** konvertieren.  
Diese Auftrennung ist sinnvoll, da Jan und Frauke anstatt Radien lieber Winkel ausgeben m√∂chten.


----


### Mittwoch, 29.11.
**13:00 bis 14:15 Uhr - Jan:**  
Die Linienerkennug mit der Hough transformation funktioniert nur unzureichend. 
Jan hat nun wieder die ADTF Demo Variante mit den Gr√ºnen Punkten aktiviert und die parameter im Video Playback angepasst. 
Die Linien werden hier gut erkannt. Als n√§chstes m√ºssen die gefundenen Punkte zu Linien zusammengefasst werden. 

### Freitag, 01.12.
**12:45 bis 13:50 Uhr - Frauke:**  
Frauke hat sich noch einmal die Bildverarbeitung angeschaut, um zu schauen, ob es nicht doch ohne die gr√ºnen Punkte geht (siehe Jan's Eintrag oben). Bisher ist aber noch nicht wirklich etwas dabei herumgekommen.

----


### Montag, 04.12.
**09:00 bis 09:20 Uhr - Frauke, Felix:**  
Unser Stick wird nicht mehr vom System erkannt. Sowohl im BIOS/Boot-Men√º als auch aus einem laufenden Linux ist dieser nicht mehr zu sehen. Anscheinend hat die **Hardware versagt**. Hermann ist informiert und k√ºmmert sich um Ersatz.  
√úbergangsweise d√ºrfen wir nun seinen *blauen* Stick verwenden.


----


### Dienstag, 05.12.
**08:00 bis 09:00 - Frauke**  
Frauke hat sich weiter mit der Trennung von Bildverarbeitungsteil f√ºr den ADTF-Filter und die Bildverarbeitung in der LaneDetection gek√ºmmert. Die beiden Dinge sind nun getrennt, OpenCV-Operationen wurden in die `bva._pp` ausgelagert.
Dar√ºber hinaus hat sie im git-Repo den Ordner `config/hoe/` zu `config/grp2/` und die Projektdatei `config/hoe/hoe.prj` zu `config/grp2/grp2.prj` umbenannt, damit es nicht zu Konflikten oder √úberschreibungen kommt, wenn wir auf Hermanns Stick arbeiten (git hat √ºbrigens ganz schlau bemerkt, dass ich den Ordner und die Datei nur umbenannt habe :) ). Das Builden funktioniert weiterhin.
Beim Ausprobieren (Ausf√ºhren in ADTF) auf dem Auto fliegt die Exception: "CUDA driver version is insufficient for CUDA runtime version in function allocate". Anscheinend ist auf Hermanns Stick eine andere Version von CUDA installert, denn auf unserem Stick hatten wir keine Probleme mit der Version. Jetzt ist die Frage, wie wir damit umgehen ...

**09:00 bis 10:50 - Franz, Frauke, Felix, Hermann**  
Hermann hat den nVidia-Treiber wieder zum laufen gebracht. Was nach langem Suchen schlussendlich geholfen hat, war das `purge`n des nVidia-Treibers und die komplette Neuinstallation. Die BVA auf der GPU funktionierte dann wieder :)
Danach haben wir uns entschieden, Franz' SSD auf Hermann's alsten USB-Stick zu klonen. Das hat dank Hermanns Hilfe auch schnell funktioniert. Danke Hermann! :)
Danach haben wir aber einen git-Konflikt ausgel√∂st, der durch uncommittete √Ñnderungen von Franz kam.

**12:30 bis 15:20 - Alle**
In der Mittagspause wurde der Stick an Fraukes Laptop angeschlossen, damit wir ihn dort booten k√∂nnen und das "kaputte" git wieder gerade zu biegen.
Oben in der Robotik gab es wieder den gleichen CUDA-Fehler wie vorher. Hermann hat das wieder geradegebogen. Danke Hermann x2.
Lenkwinkel neu vermessen, Hermann hillft bei NVidia Treibern, Aufnahme.

**15:20 bis 17:00 - Frauke, Jan**  
Es wurde weiter an der Linienerkennung und der B√ºndelung der Linien gearbeitet. Nachdem die Parameter verstanden und angepasst wurden, funktionierte dies auch recht gut. 


----

### Mittwoch, 06.12.
**11:00 bis 13:30 - Jan**  
Das verarbeitete canny Bild `contours` wird nun in eine Vogelperspektive mit Hilfe von  `cv::cuda::warpPerspective()` verzerrt.
In Versuchen sieht das Resultat besser aus, wenn man das Warping auf das `contours` anstatt auf `src` anwendet. Dies muss aber nicht immer so sein...

Die ROI Eingrenzung muss nun warscheinlich nicht mehr durchgef√ºhrt werden, da das Bild nun eh gr√∂√ütenteils nur die Fahrbahn abbildet.
Da parallele Fahrstreifen nun auch im Bild parallel sind, wird in der Funktion `isEqual()` nun auch die Distanz ber√ºcksichtigt. 
Aus den `clusteredLines` wird nun mit Hilfe von `getAngle()` provisorisch ein erster Lenkwinkel auf der Konsole ausgegeben.  
Dieser ist jedoch noch **anf√§llig f√ºr ausrei√üende Linien**.
Eine Idee zur L√∂sung des Problems ist, die Hough-Transformation anzupassen. Eine Vernachl√§ssigung dieser Ausrei√üer k√∂nnte Probleme bei Kurven verursachen.

Ein Kommentar von Frauke: Vielleicht k√∂nnen wir **einen Median-Filter** verwenden, um die Ausrei√üer loszuwerden?


----

### Donnerstag, 07.12.
**10:30 - 12:00 Uhr - Felix, Franz**  
Zuerst haben wir das Repository wieder auf den Benutzer des Autos umgestellt. Jetzt funktioniert es wieder einwandfrei. 
Da noch einige commits auf dem Auto waren, wir aber schon zuhause weiter gemacht hatten, haben wir das lokale Repository auf den origin zur√ºckgesetzt. Dazu sind die nicht hochgeladenen commits in den lokalen branch `bva-save` gesichert worden.

Danach haben wir noch den aktuellen Stand der bva getestet in dem wir das Auto auf die Stra√üe gestellt haben. Um den Lenkwinkel anzuzeigen haben wir mit OpenCV den aktuellen Lenkwinkel in die Ausgabe gemalt.

Felix hat dar√ºberhinaus die **Lineare Funktion** fertiggestellt. Sie hat 3 Inputs (`x`: Input, `g`: Gain, `o`: Offset) und einen Output (y).  
Der Output wird wie folgt berechnet: `y = gx + o`  
Sowohl Gain als auch Offset k√∂nnen durch Parameter in der Filterkonfiguration gesetzt und √ºberschrieben werden.

Dieser Filter soll sp√§ter eine M√∂glichkeit bieten, um die Geschwindigkeit innerhalb von Kurven herabzusetzen:
    
    Input = Geschwindigkeit
    Gain = Lenkwinkel
    Offset = 0

**17:00 - 19:00 Uhr - Felix**
Am Abend habe ich einen Median-Filter erstellt. Dieser f√ºhrte allerdings noch zu einem **Segmentation Fault**.  
Der Filter besitzt eine variable Fenstergr√∂√üe, die auch **live** w√§hrend des Betriebes √ºber die Filterparameter angepasst werden kann.


----


### Freitag, 08.12.
**14:20 - 16:00 Uhr - Felix, Franz**  
Gemeinsam haben wir die Ursache f√ºr den SegFault ausfindig machen k√∂nnen und ihn behoben. Letztendlich war es nur eine umgekehrte Subtraktion, was zu einem **Zugriff auf negative Array-Indizes** f√ºhrte.  
Dies war leicht behoben und der Filter funktionierte hervorragend.

Wir setzen diesen Filter jetzt hinter der BVA f√ºr den Lenkwinkel ein. **Eine gute Fenstergr√∂√üe scheint ~10 zu sein.**

Dann haben wir uns die Motorsteuerung genauer angesehen.
Den Code von Audi kann man echt in die Tonne treten. Er ist **durchzogen von Fl√ºchtigkeitsfehlern** und hat un√ºbersichtlich **viele Redundanzen**.

Der wohl sch√∂nste Fehler ist, dass wenn das Auto sich der genw√ºnschten Geschwindigkeit ann√§hert, die berechnete Regelabweichung dich vergr√∂√üert.
**Excuse me, Sir:** *What the \*\*\*\*?*

Es ist uns r√§tselhaft wie Audi da sagen kann, dass die Werte schon ganz gut passen und der Regler vern√ºnftig funktionieren w√ºrde.

**16:00 - 19:00 Uhr - Frauke, Franz, Felix**  
Am Nachmittag haben wir Frauke die Auswirkung des Median-Filters gezeigt und weiter an der BVA rumgeschraubt.


----


### Montag, 11.12.
**14:20 - 15:20 Uhr - Felix, Franz, Frauke, Jan**  
Wir haben uns weitere Fortschritte der BVA angesehen und Franz' Regler weiter verbessert.

**15:20 - 18:20 Uhr - Frauke, Jan**  
Neues Konzept f√ºr die BVA. Es werden nun die **schwarz-wei√ü-schwarz √úberg√§nge** markiert und aus den entstehenden Punkten die Linien erstellt. Wir erhoffen uns von dieser Methode, dass weniger 'FalsePositives' auftreten.

Grunds√§tzlich **klappt der Ansatz** schon ganz gut. Lediglich die **gestrichelten Leitlinien** werden noch etwas schlecht erkannt. Hier m√ºssen die Parameter noch etwas getweakt werden.

Es gibt verschiedene Versionen, die wir ausprobiert haben:

**1)** Die Hough Line Transformation wird auf dem Bin√§rbild angewandt, nachdem ein Canny-Filter angewandt wurde. Ergebnis sind 2 - 3 (manchmal sieht man die linke Linie nicht) geb√ºndelte Linien wie in folgendem Bild:
```
/¬¶| bzw. / \
je nach Kameraposition und -ausrichtung
```
Das Problem hier ist, dass auch **viele Linien im Hintergrund** erkannt werden (z.B. Fenster/ St√ºhle). Dieses Problem soll mit Herangehensweise **1a)** und **1b)** gel√∂st werden. Ein weiteres Problem ist, dass die **gestrichelte Mittellinie meistens nur schwach** (= kleines Gewicht nach der B√ºndelung) oder gar nicht erkannt wird. Dieses Problem propagiert sich auf Varianten **1a)** und **1b)**.

**1a)** Wir w√§hlen uns mit einer Trapezform eine bin√§re Maske, die wir auf das Bin√§rbild der Stra√üe anwenden. Das funktioniert sehr gut bei geraden St√ºcken. Bei Kurven hingegen wird auch ein Teil der Kurve **abgeschnitten**.

**1b)** Wir transformieren das mit mit `cv::warpPerspective` auf eine Art **"Vogelperspektive"**. Das Ergebnis sieht so √§hnlich aus wie folgendes Bild:
```
|¬¶| bzw. ¬¶|
je nach Kameraposition und -ausrichtung
```
Vorteil: Der Lenkwinkel ist relativ leicht zu berechnen (ist dann aber auch etwas vereinfacht).
Nachteil: Wieder wird die Mittellinie nicht immer zuverl√§ssig erkannt.

**2)** Wir nutzen die von Audi/ADTF vorgegebene LaneDetection, die in einem Interessenbereich (Region of Interest, ROI) Zeilenweise nach **Intensit√§tsspr√ºngen** von Schwarz zu Wei√ü und zur√ºck sucht und dort einen Punkt setzt, wo ein solcher Intensit√§tssprung erkannt wurde. Auf diese Punkte wenden wir wiederrum die Hough Line Transformation an und haben ein √§hnliches Resultat wie in **1), nur mit weniger "Rauschen"** im Bild, sodass tats√§chlich vor allem nur die Fahrbahnmarkierungen erkannt werden. Trotzdem ist die **Erkennung der Mittellinie weiterhin ein Problem**. Auch auf dieses Bild k√∂nnte man zur leichteren Winkelberechnung eine Perspektiven-Transformation wie in **1b)** beschrieben, anwenden.

Leider liefert keine der Varianten besonders gute Ergebnisse. Die Herangehensweise in **2)** liefert allerdings schon einmal ein viel sauberes Bild als in **1)**. Ein Problem, welches beiden Varianten gemein ist, ist, dass die Linien auf dem "untransformierten" Bild leider noch **zu flach erkannt** werden, weshalb u.A. extreme Lenkwinkel ausgegeben werden (**TODO -> Bild einf√ºgen**). Au√üerdem ist in beiden Varianten die gestrichelte Mittellinie ein Problem.

**Eine Idee, die Frauke gerade beim Tippen kam:** Vielleicht k√∂nnen wir die Mittellinie auch durchgezogen lassen, damit wir zumindest testweise damit arbeiten k√∂nnen? Hermann hat gesagt, dass wir uns an einigen Stellen Sachen leichter machen k√∂nnen. Durchgezogene Mittellinien sind ja auch nicht ungew√∂hnlich.

Dar√ºber hinaus haben wir die erkannten Linien nach der **B√ºndelung mit einem Gewicht** versehen, welches angibt, wie viele Linien zu einer Linie zusammengefasst wurden. Bei der Darstellung haben wir die **Linienbreite einer Linie entsprechend ihrer Gewichtung** (= Anzahl zusammengefasster Linien) berechnet, damit das ganze anschaulich wird.

Au√üerdem kam Hermann mit einem P√§ckchen vorbei, in dem sich die neue **Basler-Kamera** befand! Wir haben sie ausgepackt und sofort ausprobiert. Die Anbindung an ADTF hat super geklappt: Man muss nur den `BaslerCamera`-Block in die Konfiguration ziehen (z.B. indem man den `RealsenseCamera`-Block ersetzt) und nach Neustart der Konfiguration usw. auf Play dr√ºcken: Das Kamerabild wird ohne weitere Treiberinstallation o.√Ñ. angezeigt, da die Treiber schon vorinstalliert waren. Leider haben wir festgestellt, dass das **Sichtfeld der Kamera stark eingeschr√§nkt ist** (ca. ~10¬∞, entgegen der vermeintlichen 50 (?)¬∞): Die mitgeliferte Linse hat eine Brennweite von 16mm. Hermann hat sich daran gemacht, weitere Objektive mit Brennweiten von 8 und 4 mm zu bestellen. Die R√ºckkamera hat eine Brennweite von 8mm. In der Zwischenzeit k√∂nnen wir uns Gedanken machen, **wo und wie wir die Basler-Kamera am Auto befestigen** k√∂nnen (vorne wie die Realsense? Schr√§g √ºber dem Auto mit ca 45¬∞-Blick auf die Stra√üe? √úber dem Auto, parallel zur Stra√üe?). Au√üerdem stellt sich die Frage bei der Platzierung √ºber dem Auto, ob und wie stark die Kamera dort **wackeln** k√∂nnte.

**18:20 - 19:15 Uhr - Felix, Franz**  
Wir haben kurz eine Sprungfunktion f√ºr die Motorregelung aufgenommen.
Gemessen haben wir Beschleunigungswerte von bis zu **0.5G beim Anfahren** und sogar bis zu **0.7G bei einer Vollbremsung**. Ganz sch√∂n betr√§chtlich.
Die H√∂chstgeschwindigkeit lag relativ konstant bei 4.8m/s.

Was uns aufgefallen ist: Die **Z-Beschleunigung fluktuiert stark**. Dies liegt wohl an dem nicht ganz ebenen Fliesenboden.
Hier wird Thorger gute Werte f√ºr seinen Accelerometer basierten Nothalt finden.

*√úbrigens:* Donuts sind auch m√∂glich. H√∂h√∂ ;-)


----

### Dienstag, 12.12.
**09:00 - 10:50 Uhr - Felix, Franz, Frauke**  
Wir haben das Problem der zu flachen Linien in der BVA erkannt. Da k√∂nnte unseres Erachtens daran liegen, dass die Kamera **nicht richtig kalibriert** wurde. Also haben wir uns den ADTF-Filter `CameraCalibration` geschnappt und mit der Konfiguration f√ºr die Realsense-Kamera gef√ºttert. Leider st√ºrzt ADTF nach dem Start aufgrund eines in dem `CameraCalibration`-Filter ausgel√∂sten **OpenCV-Assertion-Errors** ab. In der Hoffnung, dieses Problem eventuell mit dem erneuten Komplieren der ADTF-Standard-Filter l√∂sen zu k√∂nnen, haben wir das gemacht.

Wir haben das ADTF kaputt gemacht, indem wir `./build_all_eclipse` ausgef√ºhrt haben. Genauer gesagt wurden geraume Komponenten als `not available` und in Rot angezeigt. Wie sich sp√§ter herausgestellt hat, **muss man die ADTF-Standard-Filter mit GCC Version 4.x kompilieren**, und nicht wie wir es getan haben, mit GCC Version 5.x. Es stellte sich auch wieder die Frage, warum der Wechsel zwischen den Versionen nochmal notwendig war ...?

**12:30 bis 15:20 - Felix, Franz, Frauke und Jan**  
Wir haben uns von Hermann zu unseren Problemen in der BVA beraten lassen. Er sagt: **"Macht es euch an einigen Stellen leichter"**. Wir m√∂chten also nun vielleicht im Bild die erkannten Linien/Punkte als **linke und rechte Fahrbahnmarkierung** erkennen und mithilfe dieses Wissens (hoffentlich) bessere Winkel ausgeben. Dar√ºber hinaus m√∂chte Felix nun auch in der BVA mithelfen, da er mit seinen sonstigen Aufgaben fertig ist.

Danach hat sich Felix mit der **genaueren perspektivischen Transformation** besch√§ftigt, indem er die **Kalibrierung mit einem Schachbrettmuster** vornimmt. Dazu haben wir auch das RGB-Bild der Kamera perspektivisch transformiert. Wir sind allerdings nicht fertig gewordern; Felix wollte sich das am Abend noch einmal anschauen.

Weil die Ausgabe von debug-Videostreams momentan ziemlich m√ºhselig ist, hatte Frauke folgende Idee: Man k√∂nnte auf verschiedenen Stufen die entsprechenden Bilder als `outputStream` ausgeben, sodass man sie sich im ADTF anzeigen kann. Alternativ k√∂nnte man einzelne **OpenCV-Funktionen auch als eigenen ADTF-Filter** implementieren (z.B. `cv::warpPerspective()`, `cv::threshhold` oder `cv::houghLines()` usw.), sodass wir eine Modularit√§t schaffen. Da k√∂nnen nat√ºrlich viele Filter entstehen. Vielleicht kann man auch mehrere Operationen in einem Filter zusammenfassen.

----

### Mittwoch, 13.12.
**14:00 - 15:50 Uhr - Jan**  
Jan hat den Winkel der selbstgebauten RealSense Kamerahalterung erh√∂ht, sodass diese nun besser auf die Stra√üe ausgerichtet ist. 
Dies hat zu Folge, dass das transformierte Bild nicht mehr so anf√§llig bei der Beschleunigung ist. 
Ebenfalls hat Jan danach die perspektivische Transformation angepasst. Hierf√ºr wurden zur Zeit ungenutzte Parameter im ADTF benutzt, um nicht st√§ndig neu compilieren zu m√ºssen. 
Nachdem die neuen Werte gefunden wurden, wurden sie in `findLines()` √ºbertragen. Die Sichtweite nach vorne wurde deutlich verringert, da wir nicht vorrausschauend fahren m√ºssen, sondern uns nur f√ºr den **aktuellen** Lenkwinkel interessieren


----


### Freitag, 15.12.
**07:50 bis 09:30 - Felix**  
Felix begann Hilfsfunktionen zur Klassifizierung der erkannten Linien zu schreiben. In Zukunft soll es m√∂glich sein, zwischen rechten, linken und Haltelinien zu unterscheiden.


----


### Montag, 18.12.
**14:00 - 18:30 Uhr - Franz, Felix, Frauke, Jan**  
Wir haben uns um die Klassifizierung von Linien gek√ºmmert und diese nun in unterschiedlichen Farben auf dem `Canny-Bild` ausgegeben.

Die Klassifizierung wird auf die bereits geclusterten Linien angewendet.
Es wird zwischen linken, rechten und Haltelinien unterschieden.

Zus√§tzlich hat die BVA jetzt einen Geschwindigkeitsausgang erhalten, womit es uns nun m√∂glich ist, an Haltelinien anzuhalten (**Notiz: 40% der Linien m√ºssen Haltelinien sein**).

**20:50 bis 23:00 Uhr - Felix (, Frauke)**  
Felix hat sich der Spurhaltung zugewandt. Es entstanden mehrere Hilfsfunktionen, die auf den erkannten Linien arbeiten.
Es sollte m√∂glich sein, den x-Wert einer Linie auf Basis eines zugeh√∂rigen y-Wert (und umgekehrt) zu erlangen.

Die hierzu ben√∂tigten *Winkelberechnungen* wurden mit Frauke diskutiert und verifiziert.

----


### Dienstag, 19.12.
**09:00 - 10:45 Uhr - Franz, Felix, Frauke**  
Frauke und Felix haben sich zuerst mit der Einrichtung von **Teletype‚Ñ¢Ô∏è** auseinandergesetzt. Die Verbindung √ºber das FH-Robotik-Netzwerk zwischen Felix' und Fraukes Laptop funktioniert gut -- man kann also gleichzeitig an einer File schreiben --, allerdings schafft es Teletype‚Ñ¢Ô∏è nicht, auf dem Auto eine Verbindung aufzubauen. Es k√∂nnte daran liegen, dass Teletype‚Ñ¢Ô∏è versucht einen Port zu √∂ffnen, der nicht freigeschaltet ist ...

Danach haben sie sich weiter mit der BVA und der Berechnung von Schnittpunkten von Linien mit x- und y-Werten auseinandergesetzt. Man kann also f√ºr eine Linie einen bestimmten x- oder y-Wert angeben, und bekommt den jeweils anderen Wert f√ºr die Linie zur√ºck.

**14:00 - 15:20 Uhr - Alle**  
Wir beheben den SegFault in Franz' Nothalt-Pin und k√ºmmern uns um weitere Anpassungen des Spurhalte*assistenten*.  
Wir bereiten Alles darauf vor, dass das Auto sich selbst **auf der Spur zentrieren** kann.
Dar√ºberhinaus sind wir nun soweit, dass das Auto eigenst√§ndig anh√§lt, wenn es sich sicher ist, eine Haltelinie erkannt zu haben.

**15:20 - 17:30 Uhr - Frauke, Jan**  
Da das Anhalten recht fr√ºh und abrupt erfolgt, sind nun Vorkehrungen getroffen worden, die die gew√ºnschte Geschwindigkeit mithilfe der aktuellen Position der Haltelinie im Kamerabild modulieren.
So ist ein **sanftes Abbremsen** gew√§hrleistet.

**15:30 - 17:00 Uhr - Franz, Frauke**
Wir haben das Zentrieren auf der Fahrspur getestet und eine neue Vorangehensweise √ºberlegt. Dann haben ein paar Videos aufgenommen und die VNC Verbindung auf Franz Surface ausprobiert, was sehr gut funktionierte.

Anmerkung (Felix): Zur Zeit wird dies durch eine Lineare Funktion bewerkstelligt. In Zukunft k√∂nnte hier die **Sigmoid-Funktion** (S-f√∂rmig) angewendet werden. Dadurch erlangen wir einen sanften Start gefolgt von einer *"Beschleunigung"* durch die Mitte, hin zu einem sanften Ausklingen.
