# Gruppe 2 - Gelb

Teilnehmer:

- Frauke J√∂rgens
- Jan Ottm√ºller
- Franz Wernicke
- Thorger Dittmann
- Felix Maa√ü (Gruppenleiter)


### Unsere Aufgabenstellung

- Motor- und Lenkregelung + Nothalt
- Fahrbahnerkennung (Gerade und Kurve)
- Kreuzungen erkennen und anhalten

----

## Tagebuch

### Mittwoch, 15.11.
**14 bis 17 Uhr - Alle:**  
Heute haben wir uns die Demos zu Lane- und Markerdetection angesehen.

Wir verwenden ab jetzt in unserer sample Lanedetection nur den Blau-Kanal, um das blaue Klebeband gut zu erkennen. Dies hat Hermann uns gegeben, um die Fahrbahnmarkierung zu erstellen.

Wir haben ein paar Streifen auf einen Leitz-Ordner aufgeklebt und es damit getestet.
Wir w√ºrden aber gern in Zukunft von RGB auf HSV/HSB umsteigen, damit auch unterschiedliche Blaut√∂ne erkannt werden k√∂nnen (verschiedene Lichteinwirkung).
Darum haben wir versucht, einmal die Roh-Videodaten mittels OpenCV zu exportieren. Leider haben wir uns gut 45 Minuten daran aufgehangen, dass wir den falschen Codec (HFUV) verwendet haben und es so nie zu einer Datei kam.
Schlie√ülich benutzen wir nun Motion-JPG (MJPG), welches einwandfrei funktioniert.

Frauke m√∂chte sich gern eine Testaufnahme mal mit diskretem OpenCV Code (ohne ADTF drumherum) ansehen.

Dar√ºber hinaus hat Franz sich einmal die Konfiguration der Radsensoren bzw. der Motorsteuerung genauer angeschaut.

Wir sind in der Lage die Demo zum Laufen zu bringen und es erscheinen einige Diagramme. Darunter unter anderem auch der zur√ºckgelegte Fahrtweg. Leider hat die Ordinate keine Einheit und 4000m/s sind dann doch etwas viel ;-)


----


### Donnerstag, 16.11.
**17 bis 18 Uhr - Franz, Frauke, Jan, Felix:**  
Heute wollten wir f√ºr Franz einmal richtige Werte des Autos aufnehmen, dass hei√üt Spannungen, USS, Lenkwinkel, etc..., sodass er sie zuhause abspielen kann.

Leider war es uns nicht m√∂glich, die Motoren zum laufen zu bringen. Wir haben versucht, die Fernbedienung neu zu koppeln. Dies verlief einwandfrei, **jedoch tat sich nichts**.

Alle Steckkontakte waren unseren Erachtens richtig und der Akku hatte auch 8,16 Volt. Die Sicherung war intakt. Dennoch wollte weder Servo noch Antrieb reagieren.

Felix hat Hermann dar√ºber bereits informiert.

W√§hrend Franz und Felix diese ganzen Dinge pr√ºften, haben Frauke und Jan die **Linienerkennung** √ºber den HSV-Farbraum (an welchem Jan zuhause gearbeitet hatte) fertiggestellt. Wir haben die Linienerkennung jetzt also soweit fertig, dass wir das blaue Klebeband vom Hintergrund unter bekannten Lichtverh√§ltnissen sehr gut **(Danke Jan üòá)** vom Rest trennen k√∂nnen. Mit unserem Ordner-Test waren die Ergebnisse einwandfrei!

Daraufhin haben wir (Franz, Frauke und Felix) uns daran gesetzt, dass das git repo zus√§tzlich die ADTF-Konfigurationen mit beinhaltet.

**Der erste Versuch**, es per `.gitignore` plus Whitelisting (um die ganzen anderen Ordner nicht angeben zu m√ºssen) zu l√∂sen, schlug fehl. Die `.gitignore` Datei war auch nach mehreren Versuchen noch nicht richtig konfiguriert. Es wurden lediglich die Top-Level Dateien der gewhitelisteten Ordner hinzugef√ºgt.

**Der zweite Versuch**, die gew√ºnschten Ordner auszulagern und per **symlink** wieder in f√ºr ADTF an der gew√ºnschten Stelle einzubinden, ging zun√§chst super. Die Dateien wurden gefunden und man konnte das Projekt ausf√ºhren.
Leider hatte wir ab da jedoch Probleme mit den **cmakefiles**, da sie den Build-Ordner nicht mehr finden konnten. Da in Zukunft hiermit h√∂chstwahrscheinlich noch mehr Probleme aufgetreten w√§ren, haben wir diese "L√∂sung" beerdigt.

**Zuletzt** haben Frauke und Felix also doch nochmal die `.gitignore`s ausgepackt und viel trial-and-error angewendet. Dies f√ºhrte uns dann irgendwann dazu, dass git beim Hinzuf√ºgen in einer Endlosschleife zu stecken schien.  
Wir haben uns den .git Ordner mal genauer angesehen und nach Anomalien gesucht. Dieser war inzwischen 1,7GB gro√ü. Sehr merkw√ºrdig.  
Wir suchten aber weiterhin die Doku ab und sind so von einer gitignore Konfiguration zur N√§chsten gestapft.

Wir haben den Ordner gel√∂scht und ein neues repo initialisiert. **Gleicher Fehler**: `git add` f√ºhrt zu keiner Ausgabe.

Tats√§chlich sind wir erst nach ca. eine Stunde suchen der Dateigr√∂√üe des repos auf den Grund gegangen. Es war das Problem, dass wir **3GB Zeitaufnahmen** gemacht hatten, und git diese auch zur Versionierung hinzuf√ºgen wollte. Dass dies etwas dauern kann, ist verst√§ndlich.

Letztendlich haben wir diese Dateien gel√∂scht und siehe da: Unsere `.gitignore` Datei war wohl doch nicht falsch. Die "Endlosschleife" war Vergangenheit.

Auf solch ein Problem f√§llt man wohl nur einmal rein ;-)

19:45 haben wir dann schlie√ülich das Licht ausschalten k√∂nnen.


----


### Freitag, 17.11.
**11:00 bis 11:35 Uhr - Hermann, Franz, Felix:**  
Heute wieder ans Auto gesetzt und uns auf Fehlersuche begeben. Diesmal war Hermann dabei. Wir haben ihm die Probleme erneut geschildert. Wir haben die Kontakte der Einspeisung, der Sicherung und die des Abnehmers gepr√ºft.
Zwischen den Polen von Sicherung und Abnehmer war kein wirklicher Durchgang zu messen (60kOhm).

**12 bis 12:30 Uhr - Timm Bostelmann, Hermann, Franz, Frauke, Felix:**  
Nach dem Essen holten wir uns Timm, den Modellbauexperten hinzu. Wir haben den Akku direkt mit dem Motorsteuerger√§t verbunden und es leuchtete auf. **Bingo! Es lag nicht an einem frittierten Steuerger√§t.**

Insofern gingen wir davon aus, dass das PCB einen Schlag weg hatte, aber tats√§chlich stellte sich nach einem **Anruf beim Hersteller**, als dieser die Teilenummer des Boards wissen wollte, **zuf√§lligerweise** heraus, dass der Arduino nicht richtig sa√ü und Hermann ihn nur wieder richtig in den Sockel dr√ºcken musste, damit alles wieder lief.

**15:00 bis 15:30 Uhr - Franz, Frauke, Felix (+ Lukas und Hendrik):**  
Wir haben Lukas und Hendrik, zwei Interessierte, bei Vortr√§gen zu KI aufgeschnappt. Wir erkl√§rten ihnen, wie das Modell in Zusammenspiel mit dem Fahrzeug funktioniert oder halt eben nicht funktinoiert ;-).
Au√üerdem haben wir unsere gew√ºnschte Aufgabenstellung ein Mal etwas konkreter formuliert.

**15:30 bis 17:45 Uhr - Franz, Felix (+ Lukas):**  

Letztendlich haben Franz und ich an der Motorsteuerung gebastelt. Wir fanden heraus, dass die `USSStructs` und `USSStructPlaus` keine Unterschiede aufweisen und auch nie im Code referenziert werden. **Das dann zu Plausibil√§tspr√ºfung ;-)**

Des Weiteren ist uns aufgefallen, dass die Wert,e die aus dem `Wheel Converter` kommen, **exponentiell wachsen**, die Geschwindigkeit zur Raddrehzahl und die zur√ºckgelegte Distanz bei konstanter Geschwindigkeit.

Wir haben den Durchmesser der Reifen neu bestimmt und die Konstanten des Umfangs im Code abge√§ndert.

Felix konzentrierte sich nebenbei auf die Bedienung und **Kommunikation mit dem Auto via VNC**. Wir k√∂nnen einen der anderen Robotik-PCs verwenden und dort `TigerVNC` installieren. F√ºr den Host (das Auto) k√∂nnen wir `x11vnc` verwenden.

Ein Test auf Felix' virtueller Maschiene war erfolgreich :).


----


### Montag, 20.11.
**14:00 bis 17:30 Uhr - Frauke, Franz, Felix, Thorger:**  
Codecs - OpenCV wurde nicht mit FFMPEG Support gebuilded, deswegen kein `HFYU`-Codec. In Zukunft verwenden wir also `MJPG`.

Franz hat uns Zugriff √ºber SSH Keys zu Github verschafft. Daf√ºr muss noch ein Whitelisting f√ºr SSH des Robolabs erfolgen. Tempor√§r verwenden wir ein Handy als Hotspot oder klinken uns per LAN ins Mitarbeiter-Netzwerk ein.

Wir verwenden nun `TightVNC` als Client und `x11vnc` als Server.
Ein Problem ist noch, dass beim Ausst√∂pseln der Monitore der VNC Client auf einen Bildschirm limitiert wird.

R√ºckfahrkamera war kurzzeitig nicht verf√ºgbar. **Reboot tut gut**.


**17:30 bis 18:00 Uhr - Frauke, Franz, Felix:**  
Wir sind am Abend noch ein wenig mit dem Auto per Fernbedienung rumgefahren. Dabei hatten wir √ºber WLAN/VNC die Kontrolle √ºber Aufnahmen, etc.


**18:00 bis 19:10 Uhr - Franz, Felix:**  
Als wir dies nun alles zum Laufen gebracht hatten, klebten einigen blaue Streifen auf den Boden. Diese repr√§sentieren nun unsere Stra√üe. Die Fahrbahn ist nach Augenma√ü **einen Meter** breit.

Dann haben wir ein Szenario **'Links Abbiegen'** aufgenommen.


----


### Dienstag, 21.11.
**09:30 bis 10:55 Uhr - Frauke, Franz, Felix:**  

- Depth-Image-Processing
- bitwise_not (schwarz ist nah, wei√ü ist fern)
- Median-Filter (not working with radius of 7 and 16bit images)

**14:00 bis 15:30 Uhr - Alle:**  

- Position der RealSense Kamera + Halter Design
- Gedanken √ºber Zeitplanung
- Thorger's VM zum Laufen bringen
- Scope-Displays zeigen falsche Werte an: exponentielles Verhalten -> Konsole zeigt lineares Verhalten

**15:30 bis 17:15 Uhr - Jan:**

Jan hat ein Model f√ºr eine Halterung der RealSense Kamera in TinkerCAD gebaut, und mit kurzer Hilfe von Timm Bostelmann gedruckt. Leider ist die Passform etwas zu eng. Ein weiterer versuch, mit angepasstem Modell erfolgt warscheinlich morgen. 


----


### Mittwoch, 22.11.
**14:00 bis 15:30 Uhr - Jan, Franz, Felix:**  

Erneuter 3D-Druck mit angepasstem Model. Diesmal k√∂nnen wir erfreut von einem **'Snug-Fit'** sprechen. :-). Bei Bedarf kann man die RealSense Kamera nun hier anbringen und kann somit mehr von der Stra√üe vor dem Auto sehen. 

Jan hat begonnen sich mit der **Hough-Transformation** zu besch√§ftigen und sie auf 'Edges' im Kamerabild angewendet.
Erste Ergebnisse sind bereits erstaunlich gut.

Franz hat seine SSD an den Robotik-PC angeschlossen und kann dort mit Hardwarebeschleunigung die Aufnahmen auch fl√ºssig abspielen.
Leider meckert der Grafiktreiber etwas rum, was dazu f√ºhrt, dass der Desktop nicht erweitert sondern nur gespiegelt werden kann. **#NichtSoGeil :-(**

Felix schaut sich die Doku einzelner Filter an und macht sich weitere Gedanken zum Lenkregler.

**15:30 bis 17:30 Uhr - Franz, Felix:**  

Franz schraubt an einem Filter, der einfach nur einen Float-Wert ausgibt.
Diese Aufgabe klingt total trivial, ist aber alles andere als leicht zu l√∂sen: **Ein Hoch auf ADTF!**  
Die Dokumentationen von existieren Dateien ist zwar existent, leider entspricht sie jedoch nicht dem Code (Kopierpastete).  
**Merke:** *"Keine Doku ist immer noch besser als falsche Doku!"*

Felix hat sich w√§hrenddessen mit dem Lenkregler und dessen Outputs besch√§ftigt. Er stie√ü auf die selben Probleme.
Wir setzten uns also zun√§chst das Ziel, die Struktur der Filter √ºberhaupt erst einmal zu verstehen.

**17:30 bis 19:10 Uhr - Felix:**  
Als Franz dann verzweifelt das Weite gesucht hatte, warf Felix einen Blick in Hermann's Doku-Ordner zur Software. Tats√§chlich gab es einen Eintrag 'Schreib deinen ersten Filter'. Dieser bestand aber mehr oder weniger aus diesen Punkten:

1. Kopiere den Demo-Ordner
2. Bennenne ihn um
3. Benenne die enthaltenen Dateien um
4. Passe die CMake an
5. Viel Gl√ºck!

Insofern -> **Nicht sehr hilfreich :-(**  
Felix musste als noch etwas weiter an seinem Filter verzweifeln. Schlie√ülich ist ihm aufgefallen, dass die Daten√ºbertragung zwischen zwei Filtern sich einer Art komplizierter `Dictionaries` bedient.

Letztendlich ist es nur ein *"Nimm mal diese Daten (Void\*) und baller sie in den Eimer dort dr√ºben!"*. Formal sieht das dann wie folgt aus:

1. Gr√∂√üe der Daten mittels `IMediaSerializer` berechnen.
2. Speicher f√ºr ein `IMediaSample` alloziieren.
3. Einen `Write-Lock` holen. Man bekommt einen `IMediaCoderExt`
 zur√ºck.
4. Eine ID f√ºr den *'Eimer'*, in den die Daten geschrieben werden k√∂nnen, vom `IMediaCoderExt` besorgen.
5. Schreibe **diesen Wert** in **diesen Eimer**.
6. Setze die aktuelle Zeit in das `IMediaSample` ein, damit jeder wei√ü, wann das Paket zugeschn√ºrt wurde.
7. √úbertrage das `IMediaSample`.


----


### Donnerstag, 23.11.
**17:00 bis 18:30 Uhr - Felix:**  

**17:00 bis 19:15 - Franz und Frauke**

Da wir heute aufgrund des anstehenden Besuchs der Ministerin im Projektraum gearbeitet haben und wir unsere blaue Stra√üe nicht mitgenommen haben, mussten wir rot/orange Riesen-Steckerleisten zum Testen zu Rate ziehen. F√ºr die Erkennung der Farben mussten wir nur an den Parametern `hueLow` und `hueHigh` schrauben. Wir haben den Bereich zwischen 0 und 10¬∞ gew√§hlt.

Wir haben uns mit der GPU-Implementierung der Hough-Transformation besch√§ftigt. Es stellte sich heraus, dass das gar nicht so schwierig ist, man muss nur die richtigen Header (gute Kandidaten sind `<opencv2/cudaarithm.hpp>`, `<opencv2/core/cuda.hpp>` und `<opencv2/cudaimgproc.hpp>`) kennen und in vielen F√§llen nur die `cv::Mat`'s per `upload` auf die GPU laden. Ergebnis ist eine `cv::cuda::GpuMat`. Ist man fertig mit allen GPU-Operationen, muss die `cv::cuda::GpuMat` wieder heruntergeladen werden und auf eine `cv::Mat` geschrieben werden. Vor viele Funktionen muss man auch nur `cv::cuda::` + `Funktionsname` schreiben. Fies wird es, wenn das gerade nicht reicht, und sich die komplette Signatur √§ndert! So ist das zum Beispiel bei der Funktion `cv::HoughLines`. Die wird in der GPU-Implementierung durch einen Pointer auf eine abstrakte Klasse repr√§sentiert. Eine Variable dieses Typs sieht dann so aus: `cv::Ptr<cv::cuda::HoughLinesDetector> hough`. Auf diesen Pointer kann man dann beispielsweise die Funktion `detect` aufrufen. Beide Operationen zusammen haben eine √§hnliche Signatur wie die CPU-Implementierung von `cv::HoughLines`. Einige Funktionen gibt es auch nicht mit GPU-Unterst√ºtzung, zum Beispiel `cv::line`.

Nachdem alle Kompilier- und Laufzeitfehler behoben wurden, funktionierte es auch! **Und wir haben einen deutlichen Unterschied zwischen CPU- und GPU-Implementierung gemerkt**, sodass die GPU-Variante -- je nach Aufl√∂sung des Videos -- nahezu in Echtzeit abl√§uft. Dazu k√∂nnen wir aber gerne noch genauere Messungen vornehmen. Leider haben wir die CPU-Variante nicht committed, sodass wir die erst einmal wieder rekonstruieren m√ºssen :-(.

Jetzt steht nur noch die Frage im Raum, wie man damit arbeiten kann, wenn man keine nVidia-Grafikkarte, oder keinen station√§ren PC hat (wie Frauke). Implementiert man erst f√ºr die CPU, und muss man dann alles √§ndern? Oder implementiert man "blind" in der GPU-API und testet dann Live am Auto? Vielleicht haben wir uns damit doch selbst ins Bein geschossen...


----


### Freitag, 24.11.
**16:00 bis - Felix, Franz und Frauke:**

Wir haben Franz die GPU-Implementierung gezeigt, die Farbwerte wieder an Blau angepasst.

Unsere alten Werte waren:
```
hueHigh:    120
hueLow:     90
Saturation: 120
Value:      ??
```

Unsere neuen Werte sind:
```
hueHigh:    120
hueLow:     100
Saturation: 80
Value:      2
```
~~Unsere alten Werte haben wir wohl nie committed, deshalb sind sie verloren gegangen. Vielleicht wei√ü Jan sie noch ...~~ Frauke hat sie in einem Kommentar gefunden (bis auf `Value` -- siehe oben)!


----


### Montag, 27.11.
**10:00 bis 14:00 Uhr - Felix und Franz:**  
Fertigstellung des `FloatValueGenerator` und Fehlersuche f√ºr den `RadiusToAngleConverter`. Es lag letztendlich an einer nicht initialisierten Variablen :]

**14:00 bis 15:30 Uhr - Felix, Franz, Frauke und Jan:**  
Mapping von Angle auf ServoValue im `RadiusToAngleConverter` erstellt.
Frauke und Jan haben sich weiter mit der Fahrbahnerkennung besch√§ftigt. Es wurde ein Algorithmus zur B√ºndelung √§hnlicher Linien erstellt, da f√ºr eine Fahrbahnmarkierung mehrere Linien erkannt werden. Hierzu wird `cv::partition()` und eine eigene Hilfsmethode `isEqual()` verwendet. 

**15:30 bis 16:45 Uhr - Frauke und Jan:**  
S.o.

**18:30 bis 19:00 Uhr - Felix:**  
Test des `RadiusToAngleConverter`.
Inbetriebnahme des Fahrzeugs war geplant. Jedoch hat die Hardware versagt. Erst wollte Ubuntu nich mehr booten (im Recovery-Mode gings :]), danach klappte VNC nicht mehr.

Insofern muss der Test auf morgen verschoben werden.


----


### Dienstag, 28.11.
**21:00 bis 22:30 Uhr - Felix:**  
Felix hat den Converter in zwei Teile aufgetrennt. Wir k√∂nnen nun sowohl von einem **Radius zu einem Winkel**, als auch von einem **Winkel zu einem Servo-Stellwert** konvertieren.  
Diese Auftrennung ist sinnvoll, da Jan und Frauke anstatt Radien lieber Winkel ausgeben m√∂chten.


----


### Mittwoch, 29.11.
**13:00 bis 14:15 Uhr - Jan:**  
Die Linienerkennug mit der Hough transformation funktioniert nur unzureichend. 
Jan hat nun wieder die ADTF Demo Variante mit den Gr√ºnen Punkten aktiviert und die parameter im Video Playback angepasst. 
Die Linien werden hier gut erkannt. Als n√§chstes m√ºssen die gefundenen Punkte zu Linien zusammengefasst werden. 

### Freitag, 01.12.
**12:45 bis 13:50 Uhr - Frauke:**  
Frauke hat sich noch einmal die Bildverarbeitung angeschaut, um zu schauen, ob es nicht doch ohne die gr√ºnen Punkte geht (siehe Jan's Eintrag oben). Bisher ist aber noch nicht wirklich etwas dabei herumgekommen.

----


### Montag, 04.12.
**09:00 bis 09:20 Uhr - Frauke, Felix:**  
Unser Stick wird nicht mehr vom System erkannt. Sowohl im BIOS/Boot-Men√º als auch aus einem laufenden Linux ist dieser nicht mehr zu sehen. Anscheinend hat die **Hardware versagt**. Hermann ist informiert und k√ºmmert sich um Ersatz.  
√úbergangsweise d√ºrfen wir nun seinen *blauen* Stick verwenden.


----


### Dienstag, 05.12.
**08:00 bis 09:00 - Frauke**  
Frauke hat sich weiter mit der Trennung von Bildverarbeitungsteil f√ºr den ADTF-Filter und die Bildverarbeitung in der LaneDetection gek√ºmmert. Die beiden Dinge sind nun getrennt, OpenCV-Operationen wurden in die `bva._pp` ausgelagert.
Dar√ºber hinaus hat sie im git-Repo den Ordner `config/hoe/` zu `config/grp2/` und die Projektdatei `config/hoe/hoe.prj` zu `config/grp2/grp2.prj` umbenannt, damit es nicht zu Konflikten oder √úberschreibungen kommt, wenn wir auf Hermanns Stick arbeiten (git hat √ºbrigens ganz schlau bemerkt, dass ich den Ordner und die Datei nur umbenannt habe :) ). Das Builden funktioniert weiterhin.
Beim Ausprobieren (Ausf√ºhren in ADTF) auf dem Auto fliegt die Exception: "CUDA driver version is insufficient for CUDA runtime version in function allocate". Anscheinend ist auf Hermanns Stick eine andere Version von CUDA installert, denn auf unserem Stick hatten wir keine Probleme mit der Version. Jetzt ist die Frage, wie wir damit umgehen ...

**09:00 bis 10:50 - Franz, Frauke, Felix, Hermann**  
Hermann hat den nVidia-Treiber wieder zum laufen gebracht. Was nach langem Suchen schlussendlich geholfen hat, war das `purge`n des nVidia-Treibers und die komplette Neuinstallation. Die BVA auf der GPU funktionierte dann wieder :)
Danach haben wir uns entschieden, Franz' SSD auf Hermann's alsten USB-Stick zu klonen. Das hat dank Hermanns Hilfe auch schnell funktioniert. Danke Hermann! :)
Danach haben wir aber einen git-Konflikt ausgel√∂st, der durch uncommittete √Ñnderungen von Franz kam.

**12:30 bis 15:20 - Alle**
In der Mittagspause wurde der Stick an Fraukes Laptop angeschlossen, damit wir ihn dort booten k√∂nnen und das "kaputte" git wieder gerade zu biegen.
Oben in der Robotik gab es wieder den gleichen CUDA-Fehler wie vorher. Hermann hat das wieder geradegebogen. Danke Hermann x2.
Lenkwinkel neu vermessen, Hermann hillft bei NVidia Treibern, Aufnahme.

**15:20 bis 17:00 - Frauke, Jan**  
Es wurde weiter an der Linienerkennung und der B√ºndelung der Linien gearbeitet. Nachdem die Parameter verstanden und angepasst wurden, funktionierte dies auch recht gut. 


----

### Dienstag, 06.12.
**11:00 bis 13:30 - Jan**  
Das verarbeitete canny Bild `contours` wird nun in eine Vogelperspektive mit Hilfe von  `cv::cuda::warpPerspective()` verzerrt.
In Versuchen sieht das Resultat besser aus, wenn man das Warping auf das `contours` anstatt auf `src` anwendet. Dies muss aber nicht immer so sein... 
die ROI eingrenzung muss nun warscheinlich nicht mehr durchgef√ºhrt werden, da das Bild nun eh gr√∂√ütenteils nur die Fahrbahn abbildet. 
Da parallel Fahrstreifen nun auch im Bild parallel sind, wird in der Funktion `isEqual()` nun auch die Distanz ber√ºcksichtigt. 
Mit hilfe der `clusteredLines` wird nun mit Hilfe von `getAngle()` provisorisch ein erster Lenkwinkel auf der Konsole ausgegeben.
dieser ist jedoch noch anf√§llig f√ºr ausrei√üende Linien.
Eine Idee zur L√∂sung des Problems ist, die hough transformation anzupassen. Eine Vernachl√§ssigung dieser Ausrei√üer k√∂nnte Probleme bei Kurven verursachen.
Ein Kommentar von Frauke: Vielleicht k√∂nnen wir einen Median-Filter verwenden, um die Ausrei√üer loszuwerden?


----

